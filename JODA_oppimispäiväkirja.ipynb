{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osallistuin avausluennolle ja kävin läpi ensimmäisen koodiklinikan notebookin itsenäisesti. Käytän päiväkirjan laatimiseen luennolla läpikäytyjä asioita ja koodidemossa käytän oman projektini koodia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerron luennon keskeisimpiä asioita viitaten ensimmäisen luennon notebookin sisältöön. Tutustuimme Venn diagrammiin, jossa on datatieteen neljä kokonaisuutta. Todellinen unelmien datatieteilijä hallitsisi kaikki nämä osa-alueet (liiketoimintaosaaminen, ohjelmointi- ja tietokantaosaaminen, tilastollinen analyysi ja datalähtöinen viestintä ja visualisointi). Todellisuudessa kuitenkin datatiede projekteja tehdään yleensä tiimeissä, jossa jokaisella tekijällä on omat vahvuutensa eri osa-alueissa. Esim. joku tiimistä voisi olla data engineer vahvan osaamisensa takia tietokannoissa ja pilviarkkitehtuureissa. Tiimistä löytyy myös todennäköisesti tilastollista analyysia hallitseva jäsen ja product owner, jonka tehtävänä on viestiä stakeholderien kanssa ja välittää vaatimuksia tiimillensä. \n",
    "\n",
    "Luennolla käytiin läpi myös datatieteen määritelmää. Joku voisi ajatella datatieteen olevan vain tilastotiedettä koristeltuna kimalteella, mutta todellisuudessa datatiede on myös liiketoiminta-ajattelua ja esimerkiksi laajoja ohjelmointitoteutuksien tuottamista. Itse olen myös havahtunut, että koneoppimista pidetään usein vain sovellettuna tilastotieteenä. Tuotantoon pantavien koneoppimismallien kehittäminen ja jatkuva optimointi kuitenkin vaatii muutakin kuin tilasto-osaamista.\n",
    "\n",
    "Datatieteen merkitys on kasvanut ihmisten havahduttua valtaviin määriin dataa heidän ympärillään ja mahdollisuuksista etsiä datasta vastauksia liiketoiminnan kysymyksiin. Ensiksi kysytään ihmisiltä ja sen jälkeen kysellään datalta.\n",
    "\n",
    "Mooren laista on myös notebookissa mainintaa. Käyn parhaillaan concurrency-kurssia, jossa on ollut mainintaa samasta aiheesta. Vaikka kellotaajuuksien kasvu on pysähtynyt, yhä pienempiä transistoreja voidaan pakata pienemmälle alueelle mikropiiriä ja mikropiirien määrää voi myös kasvattaa jolloin laskenteho nousee transistorien määrän lisääntyessä. Tämä on johtanut myös siihen, että rinnakkainen laskenta on yleistynyt. Säikeitä jaetaan useammalle prosessointiyksikölle laskettavaksi samanaikaisesti.\n",
    "\n",
    "Luin myös perjantain koodipajan notebookin läpi. Olen iloinen siitä, että pääsin taas kertaamaan Pandasin funktionaliteetteja. Pandas on itsessään minulle hyvin tuttu työelämästä ja omista projekteisteistani, mutta muistin virkistäjänä sisältö toimi hyvin. Notebookissa käsiteltiin myyntidataa (luettiin data sisään, transformoitiin muuttujia, luotiin uusia muuttujia) ja tehtiin pienimuotoista analyysia esim. Pivot-taulukolla ja yksinkertaisella histogrammi-graafilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viisi tärkeintä oppimaani asiaa luennolta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Big dataa ympäröi kova hype, mutta oikeastaan vain isot firmat kuten Google tai Amazon pääsee niihin käsiksi (sosiaalisen median dataa). Toisaalta massadataa on myös lääketieteen puolella.\n",
    "2. Datatiede on muutakin kuin vain teknistä osaamista (liiketoimintakeskeinen ajattelu on tärkeää)\n",
    "3. Datatieteen kirjoon kuuluu valtava määrä opeteltavia asioita, mutta onneksi projekteja tehdään yleensä tiimeissä. Omiin vahvuuksiin ei välttämättä tarvitse siis kuulua kaikkia datatieteen sateenvarjon alle kuuluvia asiakokonaisuuksia.\n",
    "4. Vaikka datatiede on ollut iso juttu jo kymmenen vuotta, niin hyvin harva yritys on edes ryhtynyt muuttamaan toimintaansa datalähtöisemmäksi.\n",
    "5. 80% datatiede projektista on datasepän työtä. (ETL:lää eli datan hankkimista, puhdistamista, transformointia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kehitysideat (itselleni):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Haluan oppia kysymään oikeita kysymyksiä stakeholdereilta ja opetella ns. systeemiajattelua. Tekninen osaaminen ei ole oma pullonkaulani.\n",
    "2. Vähän kertailla Pandasia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kehitysideoita kurssiviikolle ei ole, sillä mielestäni luento ja notebookit olivat todella kattavia ja intuitiivisia. Ne toimivat siis hyvin johdatteluna varten itse aiheeseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koodidemo omasta vanhasta elinajanodote-projektista Pythonilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarvittava kirjasto datan käsittelyä varten (dataframe)\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ladataan data ohjelmaan dataframeksi ja näytetään ylimmät rivit\n",
    "df = pd.read_excel(\"FinalData.xlsx\") #Data from World Bank\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastellaan eri muuttujien jakaumia ja havaitaan outlierit\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Katsotaan muuttujien datatyypit\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastetaan puuttuvien arvojen määrät jokaisesta muuttujasta\n",
    "df.apply(lambda x: sum(x.isnull()),axis=0) #amount of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastetaan selitettävän muuttujan normaalijakautuneisuus\n",
    "df['LifeExpextancy'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutkitaan myös selitettävän muuttujan jakaumaa boxplotilla kehitysmaiden ja teollisuusmaiden välillä ja havaitaan myös\n",
    "#outliereitä selitettävässä muuttujassa\n",
    "df.boxplot(column='LifeExpextancy', by = 'DevelopedOrNot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastellaan pearsonin korrelaatiokertoimia eri muuttujien välillä\n",
    "df.corr(method='pearson') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koko koodin löytää omasta github repostani: https://github.com/Niittyv/Data-science-project-life-expectancy/blob/master/DS_project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katsoin viikon luennon Panoptosta ja kävin itsenäisesti koodiklinikan materiaalin läpi. Luin myös Harvard Business -artikkelin.\n",
    "\n",
    "Luennon aiheena oli data kerääminen ja erityisesti ryömijät ja raapijat. Alussa käytiin läpi datatieteen prosessin kaaviota (datatieteen työnkulku) ja esiin tuli ETL:n ja DAD:n eroavaisuudet. ETL (extract/transform/load) on enemmän datainsinöörien juttu ja keskittyy datan saamiseen osaksi infastruktuuria. DAD (Discover/access/distill) taas koskee enemmän datatietelijöitä. Tässä prosessissa data otetaan nopeasti haltuun ja ensimmäiset analyysit saadaan valmiiksi todella pian. Datatieteen työkaavio on hyvin syklimäinen rakenteeltaan. Kun stakeholdereille on saatu analyysien tuloksia, syntyy seurauksena usein uusia kysymyksiä ja mennään takaisin analyysivaiheeseen.\n",
    "\n",
    "Datan keräämiseen voidaan käyttää ohjelmointirajapintoja (API) tai dataa voi esimerkiksi ostaa. Myös käyttöoikeudet eri ohjelmointirajapintoihin on ostettavissa. Lisäksi nettisivuille voidaan tehdä ruudun raavintaa, jolloin esimerkiksi asiakasarviodataa saadaan suoraan yritysten nettisivuilta (kuten verkkokaupasta). Ruudun raavinnassa pitää kuitenkin ottaa eettiset kysymykset huomioon. Ruudun raavintaa voi tehdä esim scrapylla tai beautiful soupilla (Python kirjastoja). Raavinnasta tulee kuitenkin hyvin haastavaa jos webbisivut ovat dynaamisia. Saatetaan tarvita selainemulaattoria.\n",
    "\n",
    "Googlen hakukone on ryömijä. Ryömijät toimivat hakemalla netistä dokumentteja ja jälleen etenemällä dokumentista toiseen.\n",
    "\n",
    "Analytiikalle on erillaisia alalajeja:\n",
    "1. kuvaileva analytiikka pyrkii selittämään mitä on tapahtunut\n",
    "2. diagnisoiva analytiikka pyrkii selittämään miksi jotain tapahtui\n",
    "3. ennakoivassa analyytikassa on usein kyse aikasarja-aineiston mallintamisesta tulevaisuuteen\n",
    "4. ohjaava analytiikka pyrkii vastaamaan kysymykseen mitä asialle pitäisi tehdä\n",
    "\n",
    "Lopuksi käytiin läpi data wranglingia. Enimmäkseen sitä, kuinka dataa muutetaan oikeaan formaattiin (päivämäärät datetime-formaattiin). Lisäksi näytettiin hieman EDAa eli exploriittista data-analyysia. Nopealla exploriittisella analyysilla saadaan itseasiassa aika paljon hyvää tulosta ja löydöksiä heti alkuun ja siihen todellakin kannattaa käyttää aikaa sen sijasta, että heti alkaisi kehittelemään ML-malleja.\n",
    "\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "\n",
    "1. Tuotteistettu analytiikka vaatii ETL-prosessit alleen (Datan pitää olla hyvässä formaatissa jotta sitä voi käyttää analytiikkaan tuotannossa).\n",
    "2. Data science tuotteet kuuluvat myös ohjelmistotekniikan alaisuuteen. Tämä tarkoittaa siis sitä, että esim. tuotannossa olevaa koodia analyysia varten pitää myös debugata.\n",
    "3. Videoita voi upottaa jupyter notebookkiin.\n",
    "4. Harvard Business -artikkeli \"Why data science teams need generalists, not specialists\" opetti, että datatieteilijöiden tiimeissä tehokkuus ei ole tärkeintä. Tärkeintä on, että jokainen tiimin jäsen oppii mahdollisimman paljon. Datatiede-tuotteita ei voi suunnitella etukäteen, ne pitää oppia. TÄMÄ ON TÄRKEÄÄ ITSELLENI.\n",
    "5. Yrityksissä on paljon hyödyntämätöntä potentiaalia analytiikalle.\n",
    "\n",
    "Kehitysideat (luennoijalle):\n",
    "1. Demokoodi raapijoista ja ryömijöistä oli vaikeasti ymmärrettävää. En oikeen saanut siitä mitään irti. Ymmärsin kuitenkin, että API:t helpottavat datan saantia nettisivuilta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jappe\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "#request-kirjastoa käytetään julkisten API:en hyödyntämiseen\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random user generation API satunnaisten käyttäjätietojen luomiselle\n",
    "response = requests.get(\"https://randomuser.me/api/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#status-koodi 200 tarkoittaa, että API:sta datan lataaminen onnistui\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"results\":[{\"gender\":\"male\",\"name\":{\"title\":\"Mr\",\"first\":\"Albert\",\"last\":\"Gimenez\"},\"location\":{\"street\":{\"number\":1157,\"name\":\"Calle de Atocha\"},\"city\":\"La Coruña\",\"state\":\"Cantabria\",\"country\":\"Spain\",\"postcode\":19472,\"coordinates\":{\"latitude\":\"-19.3251\",\"longitude\":\"-45.9854\"},\"timezone\":{\"offset\":\"-3:30\",\"description\":\"Newfoundland\"}},\"email\":\"albert.gimenez@example.com\",\"login\":{\"uuid\":\"05416641-e8dc-4865-befc-3b349a66b76a\",\"username\":\"yellowrabbit277\",\"password\":\"twilight\",\"salt\":\"4H2UIzuS\",\"md5\":\"d5d08e7180c7bd653969ca53046c36c0\",\"sha1\":\"d6c9d1402d09d3a4fe0a8555a9aed03ccf6eadcf\",\"sha256\":\"78ebdb8c0b741274f118775ad20c3283d5423d1a0aae97fa6e7822aae26ed4b7\"},\"dob\":{\"date\":\"1983-12-06T08:35:17.524Z\",\"age\":38},\"registered\":{\"date\":\"2005-11-16T21:05:09.913Z\",\"age\":16},\"phone\":\"956-447-446\",\"cell\":\"697-342-417\",\"id\":{\"name\":\"DNI\",\"value\":\"77131742-A\"},\"picture\":{\"large\":\"https://randomuser.me/api/portraits/men/29.jpg\",\"medium\":\"https://randomuser.me/api/portraits/med/men/29.jpg\",\"thumbnail\":\"https://randomuser.me/api/portraits/thumb/men/29.jpg\"},\"nat\":\"ES\"}],\"info\":{\"seed\":\"6a860c1fc05adbb2\",\"results\":1,\"page\":1,\"version\":\"1.3\"}}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response sisältää pyydetyn datan\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tekstidata json-formaattiin\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "#tallennetaan json-data\n",
    "with open('personal.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file)\n",
    "    \n",
    "#luetaan json-data dataframeen\n",
    "df = pd.read_json('personal.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Wed, 07 Apr 2021 16:06:08 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Set-Cookie': '__cfduid=d1925885d7c07af8bb93bcda7fcf4d13a1617811568; expires=Fri, 07-May-21 16:06:08 GMT; path=/; domain=.randomuser.me; HttpOnly; SameSite=Lax', 'X-Powered-By': 'Express', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache', 'ETag': 'W/\"47c-l8w1dwRK+/dsAv9vxveEKOZs3Jw\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'CF-Cache-Status': 'DYNAMIC', 'cf-request-id': '094eae8edb0000fe3cf4a83000000001', 'Expect-CT': 'max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"', 'Report-To': '{\"group\":\"cf-nel\",\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report?s=xomAMwTZ2ajb1WCc8136MzXq8ZxL6%2BRYMU7HozhfvvOVffTyyg5%2B%2FWPvH0Cki5gjg0JIo5OuWluHtT1tMDFjqW3eX96jfKK1C0%2F2i7Z%2B\"}],\"max_age\":604800}', 'NEL': '{\"max_age\":604800,\"report_to\":\"cf-nel\"}', 'Server': 'cloudflare', 'CF-RAY': '63c4805e2c3bfe3c-HEL', 'alt-svc': 'h3-27=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request sisältää tietoja API-kyselystä (kuten URL)\n",
    "request = response.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://randomuser.me/api/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GET'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get ja Post ovat kaksi eri metodia\n",
    "request.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.24.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katsoin viikon luennon Panoptosta. Olin paikalla koodiklinikassa. \n",
    "\n",
    "Tämän viikon aiheena on ohjattu oppiminen. Ohjatussa oppimisessa datasetti siis sisältää selitettävän muuttujan arvoja. Datasetistä etsitään tärkeitä ominaisuuksia (featureita), joiden avulla voidaan ennustaa selitettävän muuttujan arvoja.\n",
    "Opetusdata sisältää sekä selitettävän ominaisuuden arvoja, että selittävien ominaisuuksien arvoja. Opetusdatalla opetetaan malli tekemään ennusteita selittävien ominaisuuksien perusteella. Testidataa ei näytetä mallille opetusvaiheessa, vaan mallia kokeillaan sellaiseen dataan, jota se ei ole ennen nähnyt. Tällä tavalla voidaan testata mallin suorituskykyä. Pelkän mallin tarkkuuden tutkiminen ei riitä overfitin takia, vaan esim. cross-validaatiota kannattaa käyttää mallin suorituskykyä arvioidessa.\n",
    "\n",
    "Feature selectionilla valitaan parhaat ominaisuudet ja feature engineeringillä yhdistellään ominaisuuksia (piirteiden jalostaminen). Kun koneoppimismallin koulutus on valmista ja ollaan tyytyväisiä mallin suorituskykyyn, voidaan se siirtää tuotantoon jossa se käsittelee ja tekee ennustuksia uudesta datasta. Omalla kokemuksellani ohjattua oppimista voidaan käyttää pelkästään jo siihen, että etsitään datasta hyviä selittäviä ominaisuuksia. Joissakin tapauksissa siis ilman tuotantoon laitettavaa mallia.\n",
    "\n",
    "Erityyppissiä piirteitä (ominaisuuksia/featureja) on esim. binäärisiä, kategorisia ja jatkuvia.\n",
    "\n",
    "Mainintana tuli \"tekoälytalvi\", josta olen kuullut aikaisemmin kurssilla nimeltä \"johdatus tekoälyyn\" Oulun yliopistolla. Kyseessä on siis jäätävä hype tekoälyn potentiaalista ja tutkimusläpimurroista: Yleensä ollaan kuitenkin petytty, kun tekoäly ei kyennytkään tarjoamaan sitä mitä odotettiin. Luennolla mainittiin, että nykyään kuitenkin isot teollisuuden firmat (Google, Amazon, jne..) kykenee tarjoamaan paljon massiivisempia määriä dataa kehittäjäyhteisölle. Tekoälyä on siis mahdollsita integroida helpommin maailmanlaajuisiin prosesseihin. \n",
    "\n",
    "Ohjatun oppimisen lisäksi on myös vahvistusoppimista, ohjaamatonta oppimista ja semi-ohjattua oppimista.\n",
    "\n",
    "Luennolla käytiin läpi NLP:tä (luonnollisen kielen prosessointia). Ensimmäisenä yritettiin koodata eniten esiintyvien sanojen lukumääriä tekstissä ja poistettiin hukkasanat (kuten a, the, an). NLP:llä on iso tulevaisuus edessään ja siihen liittyy myös monitieteisyyttä. Työkaverini teki varsin mittavaa NLP-analyysia viime kesänä ja kiinnostuin kovasti näistä menetelmistä.\n",
    "\n",
    "\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "1. Tampereen yliopistolla on huipputason konenäkö-tutkimusta. Maisterivaiheessa pääsen uppoutumaan aiheeseen paremmin.\n",
    "2. Asiakaspoistuma-analyysilla on hyvä lähteä liikkeelle. Olen tehnytkin tätä töissä jo ja tulen varmasti tekemään lisää.\n",
    "3. Machinelearningmastery on loistava nettisivu valmiiden toteutusten löytämiseen.\n",
    "4. \"raakadataa\" ei oikeastaan ole olemassa. Aina kun dataa kerätään ihmisten ja organisaatioiden toimesta, on keräämisen taustalla ihmisten olettamuksia ja heidän valintoja. Dataan siis pitää suhtautua aina kriittisesti ja tehdä huolellisesti EDAa ja tutkia tätä dataa kaikenlaisten virheiden varalta. Kannattaa katsoa jakaumia. \n",
    "5. Tekoälystä ei ole mitään hyötyä, jos yritys ei tiedä mitä on tekemässä (perusliiketoiminta ensiksi kuntoon).\n",
    "6. featureiden ja algoritmien olisi hyvä olla selkeästi selitettävissä myös business-ihmisille.\n",
    "7. Eri malleja ja skaalausmenetelmiä testattaessa voi mallit ja eri tavoilla skaalatut dataframet laittaa nätisti listoihin ja sen jälkeen for-loopissa iteroida eri menetelmät läpi eri dataframeille. Tämä on hyvin algoritminen tapa kokeilla eri mallien ja menetelmien performanssia pienellä määrällä koodia.\n",
    "\n",
    "Flippivinkit (itselleni):\n",
    "1. Tutustu data lakeen\n",
    "2. tutustu TF-IDF (NLP:tä varten)\n",
    "\n",
    "Kehitysideat (luennoitsijalle):\n",
    "1. Esimerkkikoodia API-datan lataamisesta notebookkiin olisi voinut olla tarjolla. Muutenkin jonkinlainen johdatus API-raavintaan olisi ollut paikallaan. (Esim. RapidAPI.com on hyvä nettisivu API:en ettimiseen ja antaa RapidAPI-tokenin API:en käyttöä varten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#koodidemossa hyödynnän samaa dataa omasta GitHubistani, mitä käytin ensimmäisellä viikolla\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#skaalataan data regressiomallille sopivaksi\n",
    "X = transformed['InfantDeathPer1000'].values.reshape(-1, 1)\n",
    "Y = transformed['LifeExpextancy'].values.reshape(-1, 1)\n",
    "\n",
    "#koulutusdataa ja testausdataa yhden muuttujan lineaarista regressiota varten\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "#sovitetaan lineaarinen regressiomalli\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "#tehdään ennustukset testidatalla, jota voidaan verrata todellisiin arvoihin\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "#mallin tarkkuus\n",
    "regressor.score(X_test, Y_test)\n",
    "\n",
    "#RMSE\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neljännellä viikolla katsoin luentotallenteen jälkikäteen Panoptosta. Luin myös Airbnb esimerkkikoodin, josta tein muutamia havaintoja flippivinkkeihin. Katsoin viikon koodiklinikan Panoptosta.\n",
    "\n",
    "Viikon aiheena on harjoitustyön tekeminen. Ensimmäiseksi tutkailtiin Houston Analyticsin projektimallia, jossa kokoonpano on jaettu selkeästi kahteen ryhmään (problem team ja solution team). Problem team käsittelee liikentoimintaongelmaa sillä tavoin, että Solution team kykenee vastaamaan siihen analyyttisesti iteratiivisia menetelmiä käytteän.\n",
    "\n",
    "Luennolla käytiin läpi koko harjoitustyön etenemisketjua. Harjoitustyön pohjana olisi hyvä olla laajasti preferoitu CRISP-DM projektimalli. Tutkin itsekseni CRISP-DM mallista lisää sivulta https://www.datascience-pm.com/crisp-dm-2/. CRISP-DM mallissa ehdottoman tärkeä vaihe on heti ensimmäinen: liiketoiminta ymmärrys, joka on koko DS-projektin pohjarakenne. Tulin siihen tulokseen, että mallin vesiputousmaisuudesta kannattaa päästä eroon nopeilla iteroinneilla. Ehkä itse liikentoimintakysymyskin muovautuu iteraatioiden välissä?\n",
    "\n",
    "NaBC-malli toimii hyvin data scientistien tarkoituksiin. Mietitään kuka asiakas on, mitä asiakas tarvitsee ja mikä on meidän uniikki lähestymistapa tähän tarpeeseen. Sitten mietitään, miten asiakas hyötyy meidän ratkaisusta ja miten vallalla ja olemassa olevat ratkaisut pärjäävät verrattuna meidän ratkaisuun.\n",
    "\n",
    "Kävimme läpi myös Airbnb:n yritysjohdon hosteilleen luoman viikkotiedotteen sisältöä. Pääosin viikkotiedote koskee sääntöjä ja huomautuksia. Pienissä ryhmissä yritimme etsiä liiketoimintakysymyksiä joita Airbnb:n data-analytiikkatiimi on käsitellyt ennen viikkotiedotteen julkaisua. Mietimme myös, miten poikkeusolot voisivat vaikuttaa Airbnb:n data-anlyytikoiden työhön ja heidän saatavilla olevaan dataan. Idea breakout roomeihin jakamisesta oli mielestäni hyvä, harmi ettei Flingaan tullut enempää merkintöjä.\n",
    "\n",
    "Koodiklinikalla siistittiin hieman lisää dataa lineaarista mallia varten. Puuttuvia arvoja täydennettiin joko poistamalla kaikki rivit, jotka sisältävät tyhjiä arvoja tai täydentämällä tyhjät arvot sarakkeiden mediaanien mukaan. Opin cross_val_predict:in käyttöä, jolla voi suoraa tehdä ennustukset cross-validaatiota hyödyntäen. Tämä on siistiä. Lisäksi 'k--' matplotlibissä on kiva lisä, kun halutaan piirtää nouseva malliviiva ennustettujen ja todellisten arvojen vertailuun. Mallin selityskyky parani huomattavasti kun lisää selittäjiä lisättiin malliin ja tyhjät arvot imputoitiin mediaanilla.\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "1. CRISP-DM on suosituin Data Science projektimalli syystäkin\n",
    "2. Liikentoimintaongelman ymmärtäminen ja muodostaminen on koko analyysiprosessin tärkein tukipilari\n",
    "3. Visualisoinnit eivät välttämättä ole tarpeeksi informatiivisia ja arvokkaita, etenkin jos data on massavaa\n",
    "4. Asiakas/ongelma lähtöisyys on avainasemassa kun luodaan arvoa analytiikalla\n",
    "5. Luo harjoitustyöhön kysymys, johon haluat vastata analytiikalla\n",
    "\n",
    "Flippivinkit (itselleni):\n",
    "\n",
    "1. Kun käytät CRISP-DM projektimallia, iteroi vaiheet nopeasti ja paranna laatua jokaisella iterointikerralla. Iterointikertojen välissä uusia liikentoimintakysymyksiä voi syntyä. Ekan iterointikerran jälkeen on jo tuntuma siitä, mitä uutta tai paranneltua dataa voisi tarvita tai miten mallia voi parantaa.\n",
    "2. Scikit-learnin ja Apache Sparkin voi yhdistää näppärästi\n",
    "3. Scatter matrixia voi käyttää multicollineariteettien etsimiseen muuttujien välillä\n",
    "4. GridSearchCV + Spark ovat hyvä yhdistelmä parametrien tuunaamiseen (yleisesti ottaen sklearn-spark tuo lisää potkua rinnakkaistamisen ansiosta)\n",
    "5. Käytä Jinjaa ja jQuerya dashboardin tekemiseen harkkatyöhön (tai python streamlit)\n",
    "\n",
    "Kehitysideat (luennoijalle):\n",
    "1. Idea breakout roomeista oli hyvä. Ehkä kysymyksen oppilaille voisi vielä yksinkertaistaa helpommin vastattavaksi, jotta useampi uskaltaisi vastata Flingaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# käyttämäni grid search sarimax hyperparametrien etsimistä varten. \n",
    "# cfg_list sisältää datan mahdolliset kausiluonteisuudet. Esim. arvo 7 on viikoittainen kausiluonteisuus, 12 kuukausittainen \n",
    "# ja 4 kvartaalinen kausiluonteisuus\n",
    "\n",
    "# one-step sarima forecast\n",
    "def sarima_forecast(history, config):\n",
    "\torder, sorder, trend = config\n",
    "\t# define model\n",
    "\tmodel = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n",
    "\t# fit model\n",
    "\tmodel_fit = model.fit(disp=False)\n",
    "\t# make one step forecast\n",
    "\tyhat = model_fit.predict(len(history), len(history))\n",
    "\treturn yhat[0]\n",
    " \n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    " \n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    " \n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = sarima_forecast(history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\treturn error\n",
    " \n",
    "# score a model, return None on failure\n",
    "def score_model(data, n_test, cfg, debug=False):\n",
    "\tresult = None\n",
    "\t# convert config to a key\n",
    "\tkey = str(cfg)\n",
    "\t# show all warnings and fail on exception if debugging\n",
    "\tif debug:\n",
    "\t\tresult = walk_forward_validation(data, n_test, cfg)\n",
    "\telse:\n",
    "\t\t# one failure during model validation suggests an unstable config\n",
    "\t\ttry:\n",
    "\t\t\t# never show warnings when grid searching, too noisy\n",
    "\t\t\twith catch_warnings():\n",
    "\t\t\t\tfilterwarnings(\"ignore\")\n",
    "\t\t\t\tresult = walk_forward_validation(data, n_test, cfg)\n",
    "\t\texcept:\n",
    "\t\t\terror = None\n",
    "\t# check for an interesting result\n",
    "\tif result is not None:\n",
    "\t\tprint(' > Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    " \n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test, parallel=True):\n",
    "\tscores = None\n",
    "\tif parallel:\n",
    "\t\t# execute configs in parallel\n",
    "\t\texecutor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n",
    "\t\ttasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n",
    "\t\tscores = executor(tasks)\n",
    "\telse:\n",
    "\t\tscores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n",
    "\t# remove empty results\n",
    "\tscores = [r for r in scores if r[1] != None]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    " \n",
    "# create a set of sarima configs to try\n",
    "def sarima_configs(seasonal=[0]):\n",
    "\tmodels = list()\n",
    "\t# define config lists\n",
    "\tp_params = [0, 1, 2]\n",
    "\td_params = [0, 1]\n",
    "\tq_params = [0, 1, 2]\n",
    "\tt_params = ['n','c','t','ct']\n",
    "\tP_params = [0, 1, 2]\n",
    "\tD_params = [0, 1]\n",
    "\tQ_params = [0, 1, 2]\n",
    "\tm_params = seasonal\n",
    "\t# create config instances\n",
    "\tfor p in p_params:\n",
    "\t\tfor d in d_params:\n",
    "\t\t\tfor q in q_params:\n",
    "\t\t\t\tfor t in t_params:\n",
    "\t\t\t\t\tfor P in P_params:\n",
    "\t\t\t\t\t\tfor D in D_params:\n",
    "\t\t\t\t\t\t\tfor Q in Q_params:\n",
    "\t\t\t\t\t\t\t\tfor m in m_params:\n",
    "\t\t\t\t\t\t\t\t\tcfg = [(p,d,q), (P,D,Q,m), t]\n",
    "\t\t\t\t\t\t\t\t\tmodels.append(cfg)\n",
    "\treturn models\n",
    " \n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = sarima_configs(seasonal=[1,4,12])\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 3 configs\n",
    "for cfg, error in scores[:3]:\n",
    "    print(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teemu Mikkosen (Solita) vierailuluento aloitettiin yleisellä keskustelulla datatieteestä, jolloin puhuttiin esim. koneoppimismallin tarkkuuden määrittämisestä. Toisena vierailuluennoitsijana oli mukana Timo Lehtonen. Teemu Mikkonen kertoi diplomityöstään Tampereen yliopistolla ja taustastaan Solitecilla. On mukava havaita, että diplomityöntekijä ja diplomityön arvioijat (Jukka ja Timo) ovat vielä tekemisissä keskenään. Voisin ehkä itsekin tehdä graduni Solitalle, jos vain mahdollista.\n",
    "\n",
    "Seuraavaksi siirryimme Solitan NLP-notebookkiin, joka on yhdistelmä ajettavaa koodia ja teoriaa. NLP:stä muutama sana: Luonnollinen kieli on luonteeltaan rakenteetonta, joten analysointia varten tarvitaan ensiksi tekstin esikäsittelyä ja piirteiden erottamista, jotta saadaan muodostettua esim. jonkun koneoppimismallin opetusdatan uusi muuttuja. Tämän takia NLP on usein vain yksi datalähde (esim. yksittäinen muuttuja) muiden datalähteiden joukossa koneoppimismallin opettamista varten.\n",
    "\n",
    "Datan esikäsittelyssä poistetaan esimerkiksi hukkasanat ja erikoismerkit ja muutetaan kaikki kirjaimet pieniksi. Voikko-kirjasto löytää sanojen perusmuodot. Esikäsittelyn ja kohinan poiston jälkeen teksti muutetaan numeeriseksi sanavektoriksi ja koneoppimismallia opetettaessa tehdään piirteiden erottelua. Esiprosessoitu data muutettiin fastText-yhteensopiviksi opetus ja testi tekstitiedostoiksi. fastTextiä voidaan käyttää sanojen käsittelyyn vektorissa.\n",
    "\n",
    "Valmis koulutettu malli ottaa syötteeksi esim. kysmyksen tekstimuodossa ja ennustaa, mille ministerille kysymys kuuluu. Mallin tarkkuus oli yllättävän hyvä, vaikka parametrin dim arvoksi syötin 3.\n",
    "\n",
    "Tärkeimmät oppimani asiat luennolta:\n",
    "1. Google collabia voi käyttää ilmaiseksi datan ja koodin jakamista varten suoritettavassa muodossa.\n",
    "2. Hukkasanoilla tarkoitetaan kohinaa luonnollisen kielen datassa (esim. preposition ja turhat sanat, jotka eivät tuo lisäarvoa analyysiin). Hukkasanat on siis tärkeä poistaa analyysia varten. Sanojen arvot kuitenkin riippuvat kontekstista, kaikilla aloilla kaikki hukkasanat eivät siis ole välttämättä samanlaisia.\n",
    "3. Kun opetuskategorioiden näytteiden määrissä on suurta eroa, tapahtuu koneoppimismallin opetuspainotuksissa eroja. Joudutaan alinäytteistämään tai ylinäytteistämään.\n",
    "4. NLP on yhdistelmä ohjattua ja ohjaamatonta oppimista. \n",
    "5. Jos mallin suorituskyky on heikko, palataan ensiksi datan esikäsittelyvaiheeseen.\n",
    "6. numpy on huomattavasti nopeampi kuin for-loopit matriisioperaatioiden nopeuden takia.\n",
    "7. API:a voidaan käyttää näppärästi mallia koulutettaessa (parametrit voidaan syöttää lomakkeeseen ja muutokset tapahtuu koodiin. Samanlaista ratkaisua käyttää RapidAPI.com, kun haetaan dataan esim. jonkun yrityksen nettisivulta).\n",
    "\n",
    "Kehitysideat:\n",
    "1. diplomityön suorittamisen mahdollisuudesta Timon ohjattavana olisi voinut mainita aiemmin luennolla, kun väkeä oli vielä paikalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuudennen viikon aiheena oli ohjaamaton oppiminen. Katsoin kaikki viikon materiaalit tallenteina. Teen tällä hetkellä töitä Data Scientistina (kesähessuna) toista kesää.\n",
    "\n",
    "Ensiksi katsottiin ohjaamattoman oppimisen työnkulkukaaviota. Ensimmäinen vaihe on datapisteitä klusteroimalla pyrkiä luomaan muuttujia dataan. Tämän jälkeen opetetaan malli. Uudesta datasta otettu uusi datapiste luokitellaan johonkin klusteriin ja tehdään ennustus klusterin piirteiden perusteella. Ohjaamattomassa oppimisessa ei ole valmiiksi tiedossa ennustettavaa muuttujaa. Itselläni on KMeans-klusterointi algoritmista kokemusta Tampereen yliopiston kurssilta 'data-intensive-programming'. Käytin Apache Sparkia klusteroimiseen. Piirsin kuvaajan myös kyynärpää mallista.\n",
    "\n",
    "Kagglen elokuvadatan klusteroiminen KMeans-algoritmilla ei tuottanut hyvää tulosta. Syynä on sopimaton data (vote_average-muuttuja, joka on normaalijakautunut). \n",
    "\n",
    "Seuraavaksi tarkasteltiin tekstidataa (aihemallinnusta). Aihemallinnus on itselleni täysin uusi asia ja sen sulattamiseen menee vielä aikaa. On todella subjektiivista, pidetäänkö muodostuneita aihekategorioita mielekkäinä.\n",
    "\n",
    "Koodiklinikalla keskityttiin datan ulottuvuuksien vähentämiseen (PCA:lla). \n",
    "\n",
    "Tärkeimmät oppimani asiat luennolta:\n",
    "\n",
    "1. NLP-keissit eivät ole triviaaleja teollisuudessa\n",
    "2. Min Max scalerin kanssa pitää olla varovainen, jos muuttujassa on outliereitä\n",
    "3. Outlierit ovat myös vaarallisia KMeans-algoritmia käytettäessä\n",
    "4. Teksistä etsittävien aihemäärien etsimiseen on erilaisia kirjastoja käytössä ja erilaisia tilastollisia menetelmiä käytössä.\n",
    "5. Dataa analysoidessa pitää miettiä, mitä dataa se on. (esim. elokuvaa kuvaavaa tekstiä analysoidessa. Data ei välttämättä kerro elokuvan sisällöstä vaan kuvauksen sisällöstä)\n",
    "6. Ohjaamattomassa oppimisessa tapahtuu myös ohjattua oppimista\n",
    " \n",
    "\n",
    "Kehitysideat:\n",
    "\n",
    "1. En oikein ymmärtänyt, että miksi elokuvadatan klusterointi ei tuottanut hyvää tulosta. Mitä vikaa datassa oli?\n",
    "2. Koodiklinikka meni laajalti ohi itseltäni PCA:n osalta. Tiedän entuudestaan, mitä PCA on, mutta koodia oli vaikea ymmärtää."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "# Standardointi PCA:ta varten (Käytössä Iris-datasetti)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['target']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Muutetaan 4-sarakkeinen data 2-ulotteiseksi\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
