{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osallistuin avausluennolle ja kävin läpi ensimmäisen koodiklinikan notebookin itsenäisesti. Käytän päiväkirjan laatimiseen luennolla läpikäytyjä asioita ja koodidemossa käytän oman projektini koodia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerron luennon keskeisimpiä asioita viitaten ensimmäisen luennon notebookin sisältöön. Tutustuimme Venn diagrammiin, jossa on datatieteen neljä kokonaisuutta. Todellinen unelmien datatieteilijä hallitsisi kaikki nämä osa-alueet (liiketoimintaosaaminen, ohjelmointi- ja tietokantaosaaminen, tilastollinen analyysi ja datalähtöinen viestintä ja visualisointi). Todellisuudessa kuitenkin datatiede projekteja tehdään yleensä tiimeissä, jossa jokaisella tekijällä on omat vahvuutensa eri osa-alueissa. Esim. joku tiimistä voisi olla data engineer vahvan osaamisensa takia tietokannoissa ja pilviarkkitehtuureissa. Tiimistä löytyy myös todennäköisesti tilastollista analyysia hallitseva jäsen ja product owner, jonka tehtävänä on viestiä stakeholderien kanssa ja välittää vaatimuksia tiimillensä. \n",
    "\n",
    "Luennolla käytiin läpi myös datatieteen määritelmää. Joku voisi ajatella datatieteen olevan vain tilastotiedettä koristeltuna kimalteella, mutta todellisuudessa datatiede on myös liiketoiminta-ajattelua ja esimerkiksi laajoja ohjelmointitoteutuksien tuottamista. Itse olen myös havahtunut, että koneoppimista pidetään usein vain sovellettuna tilastotieteenä. Tuotantoon pantavien koneoppimismallien kehittäminen ja jatkuva optimointi kuitenkin vaatii muutakin kuin tilasto-osaamista.\n",
    "\n",
    "Datatieteen merkitys on kasvanut ihmisten havahduttua valtaviin määriin dataa heidän ympärillään ja mahdollisuuksista etsiä datasta vastauksia liiketoiminnan kysymyksiin. Ensiksi kysytään ihmisiltä ja sen jälkeen kysellään datalta.\n",
    "\n",
    "Mooren laista on myös notebookissa mainintaa. Käyn parhaillaan concurrency-kurssia, jossa on ollut mainintaa samasta aiheesta. Vaikka kellotaajuuksien kasvu on pysähtynyt, yhä pienempiä transistoreja voidaan pakata pienemmälle alueelle mikropiiriä ja mikropiirien määrää voi myös kasvattaa jolloin laskenteho nousee transistorien määrän lisääntyessä. Tämä on johtanut myös siihen, että rinnakkainen laskenta on yleistynyt. Säikeitä jaetaan useammalle prosessointiyksikölle laskettavaksi samanaikaisesti.\n",
    "\n",
    "Luin myös perjantain koodipajan notebookin läpi. Olen iloinen siitä, että pääsin taas kertaamaan Pandasin funktionaliteetteja. Pandas on itsessään minulle hyvin tuttu työelämästä ja omista projekteisteistani, mutta muistin virkistäjänä sisältö toimi hyvin. Notebookissa käsiteltiin myyntidataa (luettiin data sisään, transformoitiin muuttujia, luotiin uusia muuttujia) ja tehtiin pienimuotoista analyysia esim. Pivot-taulukolla ja yksinkertaisella histogrammi-graafilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viisi tärkeintä oppimaani asiaa luennolta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Big dataa ympäröi kova hype, mutta oikeastaan vain isot firmat kuten Google tai Amazon pääsee niihin käsiksi (sosiaalisen median dataa). Toisaalta massadataa on myös lääketieteen puolella.\n",
    "2. Datatiede on muutakin kuin vain teknistä osaamista (liiketoimintakeskeinen ajattelu on tärkeää)\n",
    "3. Datatieteen kirjoon kuuluu valtava määrä opeteltavia asioita, mutta onneksi projekteja tehdään yleensä tiimeissä. Omiin vahvuuksiin ei välttämättä tarvitse siis kuulua kaikkia datatieteen sateenvarjon alle kuuluvia asiakokonaisuuksia.\n",
    "4. Vaikka datatiede on ollut iso juttu jo kymmenen vuotta, niin hyvin harva yritys on edes ryhtynyt muuttamaan toimintaansa datalähtöisemmäksi.\n",
    "5. 80% datatiede projektista on datasepän työtä. (ETL:lää eli datan hankkimista, puhdistamista, transformointia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flippausvinkit (en ihan ymmärtänyt tehtävänantoa, nämä ovat itselleni):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Haluan oppia kysymään oikeita kysymyksiä stakeholdereilta ja opetella ns. systeemiajattelua. Tekninen osaaminen ei ole oma pullonkaulani.\n",
    "2. Vähän kertailla Pandasia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koodidemo omasta vanhasta elinajanodote-projektista Pythonilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarvittava kirjasto datan käsittelyä varten (dataframe)\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ladataan data ohjelmaan dataframeksi ja näytetään ylimmät rivit\n",
    "df = pd.read_excel(\"FinalData.xlsx\") #Data from World Bank\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastellaan eri muuttujien jakaumia ja havaitaan outlierit\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Katsotaan muuttujien datatyypit\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastetaan puuttuvien arvojen määrät jokaisesta muuttujasta\n",
    "df.apply(lambda x: sum(x.isnull()),axis=0) #amount of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastetaan selitettävän muuttujan normaalijakautuneisuus\n",
    "df['LifeExpextancy'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tutkitaan myös selitettävän muuttujan jakaumaa boxplotilla kehitysmaiden ja teollisuusmaiden välillä ja havaitaan myös\n",
    "#outliereitä selitettävässä muuttujassa\n",
    "df.boxplot(column='LifeExpextancy', by = 'DevelopedOrNot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarkastellaan pearsonin korrelaatiokertoimia eri muuttujien välillä\n",
    "df.corr(method='pearson') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koko koodin löytää omasta github repostani: https://github.com/Niittyv/Data-science-project-life-expectancy/blob/master/DS_project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katsoin viikon luennon Panoptosta ja kävin itsenäisesti koodiklinikan materiaalin läpi. Luin myös Harvard Business -artikkelin.\n",
    "\n",
    "Luennon aiheena oli data kerääminen ja erityisesti ryömijät ja raapijat. Alussa käytiin läpi datatieteen prosessin kaaviota (datatieteen työnkulku) ja esiin tuli ETL:n ja DAD:n eroavaisuudet. ETL (extract/transform/load) on enemmän datainsinöörien juttu ja keskittyy datan saamiseen osaksi infastruktuuria. DAD (Discover/access/distill) taas koskee enemmän datatietelijöitä. Tässä prosessissa data otetaan nopeasti haltuun ja ensimmäiset analyysit saadaan valmiiksi todella pian. Datatieteen työkaavio on hyvin syklimäinen rakenteeltaan. Kun stakeholdereille on saatu analyysien tuloksia, syntyy seurauksena usein uusia kysymyksiä ja mennään takaisin analyysivaiheeseen.\n",
    "\n",
    "Datan keräämiseen voidaan käyttää ohjelmointirajapintoja (API) tai dataa voi esimerkiksi ostaa. Myös käyttöoikeudet eri ohjelmointirajapintoihin on ostettavissa. Lisäksi nettisivuille voidaan tehdä ruudun raavintaa, jolloin esimerkiksi asiakasarviodataa saadaan suoraan yritysten nettisivuilta (kuten verkkokaupasta). Ruudun raavinnassa pitää kuitenkin ottaa eettiset kysymykset huomioon. Ruudun raavintaa voi tehdä esim scrapylla tai beautiful soupilla (Python kirjastoja). Raavinnasta tulee kuitenkin hyvin haastavaa jos webbisivut ovat dynaamisia. Saatetaan tarvita selainemulaattoria.\n",
    "\n",
    "Googlen hakukone on ryömijä. Ryömijät toimivat hakemalla netistä dokumentteja ja jälleen etenemällä dokumentista toiseen.\n",
    "\n",
    "Analytiikalle on erillaisia alalajeja:\n",
    "1. kuvaileva analytiikka pyrkii selittämään mitä on tapahtunut\n",
    "2. diagnisoiva analytiikka pyrkii selittämään miksi jotain tapahtui\n",
    "3. ennakoivassa analyytikassa on usein kyse aikasarja-aineiston mallintamisesta tulevaisuuteen\n",
    "4. ohjaava analytiikka pyrkii vastaamaan kysymykseen mitä asialle pitäisi tehdä\n",
    "\n",
    "Lopuksi käytiin läpi data wranglingia. Enimmäkseen sitä, kuinka dataa muutetaan oikeaan formaattiin (päivämäärät datetime-formaattiin). Lisäksi näytettiin hieman EDAa eli exploriittista data-analyysia. Nopealla exploriittisella analyysilla saadaan itseasiassa aika paljon hyvää tulosta ja löydöksiä heti alkuun ja siihen todellakin kannattaa käyttää aikaa sen sijasta, että heti alkaisi kehittelemään ML-malleja.\n",
    "\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "\n",
    "1. Tuotteistettu analytiikka vaatii ETL-prosessit alleen (Datan pitää olla hyvässä formaatissa jotta sitä voi käyttää analytiikkaan tuotannossa).\n",
    "2. Data science tuotteet kuuluvat myös ohjelmistotekniikan alaisuuteen. Tämä tarkoittaa siis sitä, että esim. tuotannossa olevaa koodia analyysia varten pitää myös debugata.\n",
    "3. Videoita voi upottaa jupyter notebookkiin.\n",
    "4. Harvard Business -artikkeli \"Why data science teams need generalists, not specialists\" opetti, että datatieteilijöiden tiimeissä tehokkuus ei ole tärkeintä. Tärkeintä on, että jokainen tiimin jäsen oppii mahdollisimman paljon. Datatiede-tuotteita ei voi suunnitella etukäteen, ne pitää oppia. TÄMÄ ON TÄRKEÄÄ ITSELLENI.\n",
    "5. Yrityksissä on paljon hyödyntämätöntä potentiaalia analytiikalle.\n",
    "\n",
    "Flippivinkit (luennoijalle ja itselleni):\n",
    "1. Demokoodi oli vaikeasti ymmärrettävää. En oikeen saanut siitä mitään irti. Voisin katsoa raapijoista ja ryömijöistä esimerkkejä jostain muualta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jappe\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jappe\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "#request-kirjastoa käytetään julkisten API:en hyödyntämiseen\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#random user generation API satunnaisten käyttäjätietojen luomiselle\n",
    "response = requests.get(\"https://randomuser.me/api/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"results\":[{\"gender\":\"male\",\"name\":{\"title\":\"Mr\",\"first\":\"Albert\",\"last\":\"Gimenez\"},\"location\":{\"street\":{\"number\":1157,\"name\":\"Calle de Atocha\"},\"city\":\"La Coruña\",\"state\":\"Cantabria\",\"country\":\"Spain\",\"postcode\":19472,\"coordinates\":{\"latitude\":\"-19.3251\",\"longitude\":\"-45.9854\"},\"timezone\":{\"offset\":\"-3:30\",\"description\":\"Newfoundland\"}},\"email\":\"albert.gimenez@example.com\",\"login\":{\"uuid\":\"05416641-e8dc-4865-befc-3b349a66b76a\",\"username\":\"yellowrabbit277\",\"password\":\"twilight\",\"salt\":\"4H2UIzuS\",\"md5\":\"d5d08e7180c7bd653969ca53046c36c0\",\"sha1\":\"d6c9d1402d09d3a4fe0a8555a9aed03ccf6eadcf\",\"sha256\":\"78ebdb8c0b741274f118775ad20c3283d5423d1a0aae97fa6e7822aae26ed4b7\"},\"dob\":{\"date\":\"1983-12-06T08:35:17.524Z\",\"age\":38},\"registered\":{\"date\":\"2005-11-16T21:05:09.913Z\",\"age\":16},\"phone\":\"956-447-446\",\"cell\":\"697-342-417\",\"id\":{\"name\":\"DNI\",\"value\":\"77131742-A\"},\"picture\":{\"large\":\"https://randomuser.me/api/portraits/men/29.jpg\",\"medium\":\"https://randomuser.me/api/portraits/med/men/29.jpg\",\"thumbnail\":\"https://randomuser.me/api/portraits/thumb/men/29.jpg\"},\"nat\":\"ES\"}],\"info\":{\"seed\":\"6a860c1fc05adbb2\",\"results\":1,\"page\":1,\"version\":\"1.3\"}}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response sisältää datan\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Wed, 07 Apr 2021 16:06:08 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Set-Cookie': '__cfduid=d1925885d7c07af8bb93bcda7fcf4d13a1617811568; expires=Fri, 07-May-21 16:06:08 GMT; path=/; domain=.randomuser.me; HttpOnly; SameSite=Lax', 'X-Powered-By': 'Express', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache', 'ETag': 'W/\"47c-l8w1dwRK+/dsAv9vxveEKOZs3Jw\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'CF-Cache-Status': 'DYNAMIC', 'cf-request-id': '094eae8edb0000fe3cf4a83000000001', 'Expect-CT': 'max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"', 'Report-To': '{\"group\":\"cf-nel\",\"endpoints\":[{\"url\":\"https:\\\\/\\\\/a.nel.cloudflare.com\\\\/report?s=xomAMwTZ2ajb1WCc8136MzXq8ZxL6%2BRYMU7HozhfvvOVffTyyg5%2B%2FWPvH0Cki5gjg0JIo5OuWluHtT1tMDFjqW3eX96jfKK1C0%2F2i7Z%2B\"}],\"max_age\":604800}', 'NEL': '{\"max_age\":604800,\"report_to\":\"cf-nel\"}', 'Server': 'cloudflare', 'CF-RAY': '63c4805e2c3bfe3c-HEL', 'alt-svc': 'h3-27=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request sisältää tietoja API-kyselystä (kuten URL)\n",
    "request = response.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://randomuser.me/api/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GET'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.24.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katsoin viikon luennon Panoptosta. Olin paikalla koodiklinikassa. \n",
    "\n",
    "Tämän viikon aiheena on ohjattu oppiminen. Ohjatussa oppimisessa datasetti siis sisältää selitettävän muuttujan arvoja. Datasetistä etsitään tärkeitä ominaisuuksia (featureita), joiden avulla voidaan ennustaa selitettävän muuttujan arvoja.\n",
    "Opetusdata sisältää sekä selitettävän ominaisuuden arvoja, että selittävien ominaisuuksien arvoja. Opetusdatalla opetetaan malli tekemään ennusteita selittävien ominaisuuksien perusteella. Testidataa ei näytetä mallille opetusvaiheessa, vaan mallia kokeillaan sellaiseen dataan, jota se ei ole ennen nähnyt. Tällä tavalla voidaan testata mallin suorituskykyä. Pelkän mallin tarkkuuden tutkiminen ei riitä overfitin takia, vaan esim. cross-validaatiota kannattaa käyttää mallin suorituskykyä arvioidessa.\n",
    "\n",
    "Feature selectionilla valitaan parhaat ominaisuudet ja feature engineeringillä yhdistellään ominaisuuksia (piirteiden jalostaminen). Kun koneoppimismallin koulutus on valmista ja ollaan tyytyväisiä mallin suorituskykyyn, voidaan se siirtää tuotantoon jossa se käsittelee ja tekee ennustuksia uudesta datasta. Omalla kokemuksellani ohjattua oppimista voidaan käyttää pelkästään jo siihen, että etsitään datasta hyviä selittäviä ominaisuuksia. Joissakin tapauksissa siis ilman tuotantoon laitettavaa mallia.\n",
    "\n",
    "Erityyppissiä piirteitä (ominaisuuksia/featureja) on esim. binäärisiä, kategorisia ja jatkuvia.\n",
    "\n",
    "Mainintana tuli \"tekoälytalvi\", josta olen kuullut aikaisemmin kurssilla nimeltä \"johdatus tekoälyyn\" Oulun yliopistolla. Kyseessä on siis jäätävä hype tekoälyn potentiaalista ja tutkimusläpimurroista: Yleensä ollaan kuitenkin petytty, kun tekoäly ei kyennytkään tarjoamaan sitä mitä odotettiin. Luennolla mainittiin, että nykyään kuitenkin isot teollisuuden firmat (Google, Amazon, jne..) kykenee tarjoamaan paljon massiivisempia määriä dataa kehittäjäyhteisölle. Tekoälyä on siis mahdollsita integroida helpommin maailmanlaajuisiin prosesseihin. \n",
    "\n",
    "Ohjatun oppimisen lisäksi on myös vahvistusoppimista, ohjaamatonta oppimista ja semi-ohjattua oppimista.\n",
    "\n",
    "Luennolla käytiin läpi NLP:tä (luonnollisen kielen prosessointia). Ensimmäisenä yritettiin koodata eniten esiintyvien sanojen lukumääriä tekstissä ja poistettiin hukkasanat (kuten a, the, an). NLP:llä on iso tulevaisuus edessään ja siihen liittyy myös monitieteisyyttä. Työkaverini teki varsin mittavaa NLP-analyysia viime kesänä ja kiinnostuin kovasti näistä menetelmistä.\n",
    "\n",
    "\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "1. Tampereen yliopistolla on huipputason konenäkö-tutkimusta. Maisterivaiheessa pääsen uppoutumaan aiheeseen paremmin.\n",
    "2. Asiakaspoistuma-analyysilla on hyvä lähteä liikkeelle. Olen tehnytkin tätä töissä jo ja tulen varmasti tekemään lisää.\n",
    "3. Machinelearningmastery on loistava nettisivu valmiiden toteutusten löytämiseen.\n",
    "4. \"raakadataa\" ei oikeastaan ole olemassa. Aina kun dataa kerätään ihmisten ja organisaatioiden toimesta, on keräämisen taustalla ihmisten olettamuksia ja heidän valintoja. Dataan siis pitää suhtautua aina kriittisesti ja tehdä huolellisesti EDAa ja tutkia tätä dataa kaikenlaisten virheiden varalta. Kannattaa katsoa jakaumia. \n",
    "5. Tekoälystä ei ole mitään hyötyä, jos yritys ei tiedä mitä on tekemässä (perusliiketoiminta ensiksi kuntoon).\n",
    "6. featureiden ja algoritmien olisi hyvä olla selkeästi selitettävissä myös business-ihmisille.\n",
    "7. Eri malleja ja skaalausmenetelmiä testattaessa voi mallit ja eri tavoilla skaalatut dataframet laittaa nätisti listoihin ja sen jälkeen for-loopissa iteroida eri menetelmät läpi eri dataframeille. Tämä on hyvin algoritminen tapa kokeilla eri mallien ja menetelmien performanssia pienellä määrällä koodia.\n",
    "\n",
    "Flippivinkit (itselleni):\n",
    "1. Tutustu data lakeen\n",
    "2. tutustu TF-IDF (NLP:tä varten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#koodidemossa hyödynnän samaa dataa omasta GitHubistani, mitä käytin ensimmäisellä viikolla\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#skaalataan data regressiomallille sopivaksi\n",
    "X = transformed['InfantDeathPer1000'].values.reshape(-1, 1)\n",
    "Y = transformed['LifeExpextancy'].values.reshape(-1, 1)\n",
    "\n",
    "#koulutusdataa ja testausdataa yhden muuttujan lineaarista regressiota varten\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "#sovitetaan lineaarinen regressiomalli\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "#tehdään ennustukset testidatalla, jota voidaan verrata todellisiin arvoihin\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "#mallin tarkkuus\n",
    "regressor.score(X_test, Y_test)\n",
    "\n",
    "#RMSE\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viikko 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neljännellä viikolla katsoin luentotallenteen jälkikäteen Panoptosta. Luin myös Airbnb esimerkkikoodin, josta tein muutamia havaintoja flippivinkkeihin.\n",
    "\n",
    "Viikon aiheena on harjoitustyön tekeminen. Ensimmäiseksi tutkailtiin Houston Analyticsin projektimallia, jossa kokoonpano on jaettu selkeästi kahteen ryhmään (problem team ja solution team). Problem team käsittelee liikentoimintaongelmaa sillä tavoin, että Solution team kykenee vastaamaan siihen analyyttisesti iteratiivisia menetelmiä käytteän.\n",
    "\n",
    "Luennolla käytiin läpi koko harjoitustyön etenemisketjua. Harjoitustyön pohjana olisi hyvä olla laajasti preferoitu CRISP-DM projektimalli. Tutkin itsekseni CRISP-DM mallista lisää sivulta https://www.datascience-pm.com/crisp-dm-2/. CRISP-DM mallissa ehdottoman tärkeä vaihe on heti ensimmäinen: liiketoiminta ymmärrys, joka on koko DS-projektin pohjarakenne. Tulin siihen tulokseen, että mallin vesiputousmaisuudesta kannattaa päästä eroon nopeilla iteroinneilla. Ehkä itse liikentoimintakysymyskin muovautuu iteraatioiden välissä?\n",
    "\n",
    "NaBC-malli toimii hyvin data scientistien tarkoituksiin. Mietitään kuka asiakas on, mitä asiakas tarvitsee ja mikä on meidän uniikki lähestymistapa tähän tarpeeseen. Sitten mietitään, miten asiakas hyötyy meidän ratkaisusta ja miten vallalla ja olemassa olevat ratkaisut pärjäävät verrattuna meidän ratkaisuun.\n",
    "\n",
    "Kävimme läpi myös Airbnb:n yritysjohdon hosteilleen luoman viikkotiedotteen sisältöä. Pääosin viikkotiedote koskee sääntöjä ja huomautuksia. Pienissä ryhmissä yritimme etsiä liiketoimintakysymyksiä joita Airbnb:n data-analytiikkatiimi on käsitellyt ennen viikkotiedotteen julkaisua. Mietimme myös, miten poikkeusolot voisivat vaikuttaa Airbnb:n data-anlyytikoiden työhön ja heidän saatavilla olevaan dataan. Idea breakout roomeihin jakamisesta oli mielestäni hyvä, harmi ettei Flingaan tullut enempää merkintöjä.\n",
    "\n",
    "\n",
    "\n",
    "Viisi tärkeintä oppimaani asiaa luennolta:\n",
    "1. CRISP-DM on suosituin Data Science projektimalli syystäkin\n",
    "2. Liikentoimintaongelman ymmärtäminen ja muodostaminen on koko analyysiprosessin tärkein tukipilari\n",
    "3. Visualisoinnit eivät välttämättä ole tarpeeksi informatiivisia ja arvokkaita, etenkin jos data on massavaa\n",
    "4. Asiakas/ongelma lähtöisyys on avainasemassa kun luodaan arvoa analytiikalla\n",
    "5. Luo harjoitustyöhön kysymys, johon haluat vastata analytiikalla\n",
    "\n",
    "Flippivinkit (itselleni):\n",
    "\n",
    "1. Kun käytät CRISP-DM projektimallia, iteroi vaiheet nopeasti ja paranna laatua jokaisella iterointikerralla. Iterointikertojen välissä uusia liikentoimintakysymyksiä voi syntyä. Ekan iterointikerran jälkeen on jo tuntuma siitä, mitä uutta tai paranneltua dataa voisi tarvita tai miten mallia voi parantaa.\n",
    "2. Scikit-learnin ja Apache Sparkin voi yhdistää näppärästi\n",
    "3. Scatter matrixia voi käyttää multicollineariteettien etsimiseen muuttujien välillä\n",
    "4. GridSearchCV + Spark ovat hyvä yhdistelmä parametrien tuunaamiseen (yleisesti ottaen sklearn-spark tuo lisää potkua rinnakkaistamisen ansiosta)\n",
    "5. Käytä Jinjaa ja jQuerya dashboardin tekemiseen harkkatyöhön (tai python streamlit)\n",
    "\n",
    "Flippivinkit (luennoijalle):\n",
    "1. Idea breakout roomeista oli hyvä. Ehkä kysymyksen oppilaille voisi vielä yksinkertaistaa helpommin vastattavaksi, jotta useampi uskaltaisi vastata Flingaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
